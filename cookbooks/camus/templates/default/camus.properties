etl.destination.path=<%= node[:camus][:etl][:destination] %>
etl.execution.base.path=<%= node[:camus][:etl][:base_path] %>
etl.execution.history.path=<%= node[:camus][:etl][:history_path] %>
etl.record.writer.provider.class=<%= node[:camus][:etl][:writer] %>

camus.message.decoder.class=<%= node[:camus][:message][:decoder] %>
camus.message.timestamp.field=<%= node[:camus][:message][:timestamp][:field] %>
camus.message.timestamp.format=<%= node[:camus][:message][:timestamp][:fomat] %>

# max hadoop tasks to use, each task can pull multiple topic partitions
mapred.map.tasks=30
# max historical time that will be pulled from each partition based on event timestamp
kafka.max.pull.hrs=6
# events with a timestamp older than this will be discarded.
kafka.max.historical.days=7
# Max minutes for each mapper to pull messages (-1 means no limit)
kafka.max.pull.minutes.per.task=50

# reading from cluster <%= node[:camus][:cluster] %>
kafka.client.name=camus
kafka.brokers=<%= kafka_connect(node[:camus][:cluster]) %>
kafka.timeout.value=60000
kafka.whitelist.topics=<%= node[:camus][:topics] %>

# stops the mapper from getting inundated with Decoder exceptions for the same topic
max.decoder.exceptions.to.print=10

# controls the submitting of counts to Kafka
post.tracking.counts.to.kafka=false

# everything below this point can be ignored for the time being, will provide more documentation down the road
##########################
etl.run.tracking.post=false
kafka.monitor.tier=
etl.counts.path=
kafka.monitor.time.granularity=10

etl.hourly=hourly
etl.daily=daily
etl.ignore.schema.errors=false

etl.default.timezone=UTC
etl.output.file.time.partition.mins=60
etl.keep.count.files=false
etl.execution.history.max.of.quota=.8

kafka.client.buffer.size=8388608
kafka.client.so.timeout=60000
